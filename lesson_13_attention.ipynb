{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 13 â€” Attention Mechanism: Focusing on What Matters\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "RNN/LSTM process sequences step-by-step, but struggle with long dependencies.  \n",
    "**Attention** lets models \"focus\" on relevant parts dynamically.  \n",
    "\n",
    "ðŸ‘‰ Itâ€™s the core of Transformers (BERT, GPT).  \n",
    "Enables parallel processing, better for long text/images.  \n",
    "\n",
    "Weâ€™ll build simple attention and see WHY it revolutionizes ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e6f7",
   "metadata": {},
   "source": [
    "## 1) What is Attention?\n",
    "\n",
    "- Weights how much each input part contributes to output.\n",
    "- Like human focus: Ignore noise, attend to key info.\n",
    "\n",
    "ðŸ‘‰ WHY? Better than RNN for global dependencies (e.g., translation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7g8",
   "metadata": {},
   "source": [
    "## 2) Query, Key, Value Concept\n",
    "\n",
    "- **Query**: What weâ€™re looking for.\n",
    "- **Key**: Labels on data.\n",
    "- **Value**: Actual data.\n",
    "- Score = Query â€¢ Key; Softmax for weights; Output = weights â€¢ Value.\n",
    "\n",
    "ðŸ‘‰ WHY QKV? Flexible matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f7g8h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output: tensor([[[0.4330, 0.8487],\n",
      "         [0.6920, 0.6245],\n",
      "         [0.2837, 0.5950]]], grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple attention demo\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = k.size(-1)\n",
    "    scaled = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
    "    attn_weights = F.softmax(scaled, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    return output\n",
    "\n",
    "seq = torch.rand(1, 3, 2)  # Batch=1, seq_len=3, dim=2\n",
    "print(\"Attention output:\", scaled_dot_product_attention(seq, seq, seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i0",
   "metadata": {},
   "source": [
    "## 3) Self-Attention â€” All from Input\n",
    "\n",
    "- Q, K, V all from same input (via linear projections).\n",
    "- Allows parts to attend to each other.\n",
    "\n",
    "ðŸ‘‰ WHY self? Captures internal relations (e.g., subject-verb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "h8i9j0k2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        return scaled_dot_product_attention(q, k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l3",
   "metadata": {},
   "source": [
    "## 4) Multi-Head Attention\n",
    "\n",
    "- Multiple attention heads in parallel.\n",
    "- Concat and project results.\n",
    "\n",
    "ðŸ‘‰ WHY multi? Captures different relations (e.g., syntax vs semantics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "j0k1l2m4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_dim // num_heads) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n5",
   "metadata": {},
   "source": [
    "## 5) Practice: Simple Attention Model\n",
    "\n",
    "- Use for sequence classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "l2m3n4o6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.69\n"
     ]
    }
   ],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, output_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(input_dim, embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.attn(x)\n",
    "        x = x.mean(dim=1)  # Global avg pool\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dummy training\n",
    "X = torch.rand(32, 10, 5)  # Batch=32, seq=10, dim=5\n",
    "y = torch.randint(0, 2, (32,))\n",
    "model = AttentionModel(5, 64, 4, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/50, Loss: {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p7",
   "metadata": {},
   "source": [
    "## 6) Practice Exercises\n",
    "\n",
    "- Add mask to attention (for padding).\n",
    "- Build encoder with attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "n4o5p6q8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Masked attention\n",
    "def masked_attention(q, k, v, mask=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r9",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "âœ… What we learned:\n",
    "- Attention for focus.\n",
    "- QKV mechanism.\n",
    "- Self & multi-head.\n",
    "\n",
    "ðŸš€ Next Lesson: **Transformer Architecture** â€” combining attention layers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}