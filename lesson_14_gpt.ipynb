{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 14 â€” Transformer Architecture: The Powerhouse of Modern AI\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "Attention is powerful, but Transformers combine it with other blocks to create scalable models.  \n",
    "Transformers revolutionized AI (GPT, BERT, Vision Transformers).  \n",
    "\n",
    "ðŸ‘‰ They process sequences in parallel, handle long contexts better than RNN/LSTM.  \n",
    "Weâ€™ll build a mini Transformer and see WHY itâ€™s efficient for tasks like translation or generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e6f7",
   "metadata": {},
   "source": [
    "## 1) Multi-Head Attention â€” Review & Importance\n",
    "\n",
    "- Multiple attention heads capture different aspects.\n",
    "- Concat and project for rich representation.\n",
    "\n",
    "ðŸ‘‰ WHY multi-head? Like ensemble: Diverse views of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e6f7g8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        q = self.q_linear(q).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(bs, -1, self.num_heads * self.d_k)\n",
    "        return self.out_linear(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7g8h9",
   "metadata": {},
   "source": [
    "## 2) Positional Encoding â€” Adding Order\n",
    "\n",
    "- Transformers lack built-in order (parallel).\n",
    "- Add sine/cosine waves to embeddings.\n",
    "\n",
    "ðŸ‘‰ WHY? Encodes position info without recurrence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6g7h8i0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k2",
   "metadata": {},
   "source": [
    "## 3) Encoder-Decoder Structure\n",
    "\n",
    "- **Encoder**: Stack of attention + feed-forward (process input).\n",
    "- **Decoder**: Similar + masked attention + cross-attention (generate output).\n",
    "\n",
    "ðŸ‘‰ WHY encoder-decoder? Seq2seq tasks (input â†’ output sequences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "i9j0k1l3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        src2 = self.self_attn(src, src, src, mask)\n",
    "        src = src + self.norm1(src2)\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.norm2(src2)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m4",
   "metadata": {},
   "source": [
    "## 4) Building a Mini Transformer\n",
    "\n",
    "- Embed + Positional + Encoder/Decoder stacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "k1l2m3n5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=128, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "        # Similar for decoder...\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.src_embed(src) * math.sqrt(src.size(-1))\n",
    "        src = self.pos_enc(src)\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        return self.fc_out(src)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o6",
   "metadata": {},
   "source": [
    "## 5) Practice: Mini Transformer for Sequence Tasks\n",
    "\n",
    "- Simple copy task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m3n4o5p7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "# Dummy data: Copy sequence\n",
    "vocab_size = 10\n",
    "src = torch.randint(0, vocab_size, (32, 10))  # Batch=32, seq=10\n",
    "tgt = src.clone()  # Target = input\n",
    "\n",
    "model = MiniTransformer(vocab_size, vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q8",
   "metadata": {},
   "source": [
    "## 6) Practice Exercises\n",
    "\n",
    "- Add decoder layer.\n",
    "- Use for toy translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "o5p6q7r9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Add mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s0",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "âœ… What we learned:\n",
    "- Multi-head attention.\n",
    "- Positional encoding.\n",
    "- Encoder-decoder.\n",
    "- Mini Transformer build.\n",
    "\n",
    "ðŸš€ Next Lesson: **Generative Models Basics** â€” creating new data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}