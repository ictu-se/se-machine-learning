{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 9 â€” Long Short-Term Memory (LSTM): Advanced Sequence Modeling\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "RNNs are great for sequences but suffer from **vanishing gradients** in long data (forget early info).  \n",
    "LSTMs fix this with **gates** to control memory.  \n",
    "\n",
    "ðŸ‘‰ LSTMs are used in translation (Google Translate), speech (Siri), and time series.  \n",
    "Theyâ€™re a step toward Transformers (which also handle long dependencies).  \n",
    "\n",
    "Weâ€™ll build an LSTM and compare to simple RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e6f7",
   "metadata": {},
   "source": [
    "## 1) What is an LSTM?\n",
    "\n",
    "- LSTM = RNN + **Cell state** (long-term memory) + Gates.\n",
    "- Gates decide what to remember/forget.\n",
    "\n",
    "ðŸ‘‰ WHY better than RNN? Handles long sequences without gradient issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7g8",
   "metadata": {},
   "source": [
    "## 2) Gates Mechanism â€” Forget, Input, Output\n",
    "\n",
    "- **Forget gate**: Decides what to discard from cell state.\n",
    "- **Input gate**: Adds new info to cell state.\n",
    "- **Output gate**: Decides what to output from cell state.\n",
    "\n",
    "ðŸ‘‰ Equations simplified: Use sigmoid for gates (0-1 decisions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f7g8h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states: tensor([[[-0.1688],\n",
      "         [-0.2836],\n",
      "         [-0.0360],\n",
      "         [ 0.2468]]], grad_fn=<StackBackward0>)\n",
      "Cell states: tensor([[[-0.3353],\n",
      "         [-0.9285],\n",
      "         [-0.1833],\n",
      "         [ 0.6696]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple LSTM demo\n",
    "lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=1)\n",
    "input_seq = torch.tensor([[[1.0], [2.0], [3.0], [4.0]]])  # Batch=1, seq_len=4, features=1\n",
    "h0 = torch.zeros(1, 1, 1)  # Initial hidden\n",
    "c0 = torch.zeros(1, 1, 1)  # Initial cell\n",
    "\n",
    "output, (hn, cn) = lstm(input_seq, (h0, c0))\n",
    "print(\"Hidden states:\", output)\n",
    "print(\"Cell states:\", cn)  # Long-term memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i0",
   "metadata": {},
   "source": [
    "## 3) Cell State vs Hidden State\n",
    "\n",
    "- **Cell state**: Long-term info highway (minimal changes).\n",
    "- **Hidden state**: Short-term, used for output.\n",
    "\n",
    "ðŸ‘‰ WHY separate? Cell state preserves info over long distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k2",
   "metadata": {},
   "source": [
    "## 4) Building an LSTM Model\n",
    "\n",
    "- Similar to RNN, but with cell state.\n",
    "- For sentiment or prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "i9j0k1l3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m4",
   "metadata": {},
   "source": [
    "## 5) Training on Text Data\n",
    "\n",
    "- Example: Simple sentiment (positive/negative).\n",
    "- Use embedding for words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "k1l2m3n5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.69\n",
      "Epoch 25/50, Loss: 0.50\n",
      "Epoch 50/50, Loss: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Dummy text data (sequences of numbers as \"words\")\n",
    "X = torch.rand(10, 5, 1)  # 10 samples, seq_len=5, features=1\n",
    "y = torch.tensor([[1.], [0.], [1.], [0.], [1.], [0.], [1.], [0.], [1.], [0.]])\n",
    "\n",
    "model = SimpleLSTM(1, 20, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    output = torch.sigmoid(model(X))\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 25 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/50, Loss: {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o6",
   "metadata": {},
   "source": [
    "## 6) Practice Exercises\n",
    "\n",
    "- Use LSTM for time series forecasting.\n",
    "- Stack multiple LSTM layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "m3n4o5p7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Stacked LSTM\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q8",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "âœ… What we learned:\n",
    "- LSTM gates for long-term memory.\n",
    "- Cell vs hidden states.\n",
    "- Training on sequences.\n",
    "\n",
    "ðŸš€ Next Lesson: **Data Loading & Preprocessing** â€” preparing real datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}