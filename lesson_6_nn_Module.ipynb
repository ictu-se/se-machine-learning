{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d3e2a1",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 6 â€” PyTorch nn.Module: Building Custom Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "So far, weâ€™ve used simple models like Linear or MLP with built-in layers.  \n",
    "But to build real-world models (like CNNs, Transformers), you need to create **custom classes**.  \n",
    "\n",
    "ðŸ‘‰ `nn.Module` is the base class for all neural networks in PyTorch.  \n",
    "It handles:\n",
    "- Parameter registration (weights, biases).\n",
    "- Forward pass definition.\n",
    "- Automatic gradient tracking.\n",
    "\n",
    "Understanding this = unlocking the ability to build **any architecture**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f2b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4e5f7",
   "metadata": {},
   "source": [
    "## 1) What is nn.Module?\n",
    "\n",
    "- Itâ€™s a **container** for your modelâ€™s layers and parameters.\n",
    "- Every custom model **inherits from nn.Module**.\n",
    "- Key methods:\n",
    "  - `__init__()`: Define layers here.\n",
    "  - `forward()`: Define computation (how data flows).\n",
    "\n",
    "ðŸ‘‰ WHY?  \n",
    "PyTorch automatically tracks parameters (via `.parameters()`) for training.  \n",
    "Without it, youâ€™d have to manage gradients manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c8d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple Perceptron as nn.Module\n",
    "class SimplePerceptron(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()  # Call parent class\n",
    "        self.linear = nn.Linear(n_inputs, 1)  # Layer in init\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# Usage\n",
    "model = SimplePerceptron(2)\n",
    "x = torch.zeros(4, 2)  # Dummy input\n",
    "print(\"Prediction:\", model(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4e8a9",
   "metadata": {},
   "source": [
    "## 2) __init__() â€” Defining Layers\n",
    "\n",
    "- Here, you create sub-modules (layers) as attributes.\n",
    "- PyTorch registers them automatically for gradients.\n",
    "\n",
    "ðŸ‘‰ WHY super().__init__()?  \n",
    "It initializes the parent class, enabling features like .to(device) or .parameters().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e2f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [Parameter containing:\n",
      "tensor([[ 0.6846, -0.7125],\n",
      "        [ 1.0447,  0.3510]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2961, 0.7681], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.0735,  0.1456]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2344], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# Example: MLP with init\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass  # To be defined\n",
    "\n",
    "model = SimpleMLP()\n",
    "print(\"Parameters:\", list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7d3c2",
   "metadata": {},
   "source": [
    "## 3) forward() â€” The Computation Path\n",
    "\n",
    "- Defines how input flows through layers.\n",
    "- Called automatically when you do model(x).\n",
    "\n",
    "ðŸ‘‰ WHY not call forward directly?  \n",
    "model(x) hooks into PyTorchâ€™s autograd for tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b4e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-0.4796],\n",
      "        [-0.4796]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))  # Activation\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleMLP()\n",
    "x = torch.zeros(2, 2)\n",
    "print(\"Output:\", model(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3f5e6f4",
   "metadata": {},
   "source": [
    "## 4) Parameters vs Buffers\n",
    "\n",
    "- **Parameters**: Trainable (weights, biases) â€” updated by optimizer.\n",
    "- **Buffers**: Non-trainable (e.g., running means in BatchNorm) â€” registered with `register_buffer()`.\n",
    "\n",
    "ðŸ‘‰ WHY distinguish?  \n",
    "Buffers move with model.to(device) but arenâ€™t in .parameters() for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "h4f6e7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters count: 3\n",
      "Buffer: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class ModelWithBuffer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.register_buffer('my_buffer', torch.ones(2))  # Non-trainable\n",
    "\n",
    "model = ModelWithBuffer()\n",
    "print(\"Parameters count:\", len(list(model.parameters())))  # Only linear's params\n",
    "print(\"Buffer:\", model.my_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5g7i8g6",
   "metadata": {},
   "source": [
    "## 5) Training with nn.Module\n",
    "\n",
    "- Use model.parameters() for optimizer.\n",
    "- Call model(x) for forward.\n",
    "\n",
    "ðŸ‘‰ WHY?  \n",
    "This setup scales to complex models like Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "j6h8j9h7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "# XOR data\n",
    "X = torch.tensor([[0.,0.], [0.,1.], [1.,0.], [1.,1.]])\n",
    "y = torch.tensor([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4)\n",
    "        self.out = nn.Linear(4, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        x = self.activation(self.out(x))\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8k9l0",
   "metadata": {},
   "source": [
    "## 6) Practice Exercises\n",
    "\n",
    "- Build a custom CNN class with conv layers.\n",
    "- Add a buffer for positional encoding (hint for Transformers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "m8n9m0n1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Practice: Build a simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.fc = nn.Linear(16, 10)  # Assume flattened input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0q1p2q3",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "âœ… What we learned:\n",
    "- nn.Module as base for custom models.\n",
    "- __init__() for layers, forward() for computation.\n",
    "- Parameters (trainable) vs Buffers (non-trainable).\n",
    "- Training integrates seamlessly.\n",
    "\n",
    "ðŸš€ Next Lesson: **Convolutional Neural Networks (CNN)** â€” applying nn.Module to images.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}