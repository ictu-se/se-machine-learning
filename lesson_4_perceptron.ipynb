{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b303e60",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 4 â€” The Perceptron: First Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "So far:\n",
    "- **Linear regression** â†’ predicted continuous values.\n",
    "- But real problems often need **classification** (spam vs not spam, tumor vs healthy).  \n",
    "\n",
    "ðŸ‘‰ The **Perceptron** was the first algorithm (1958, Frank Rosenblatt) to perform binary classification using a model inspired by neurons.  \n",
    "It marks the beginning of **neural networks**.\n",
    "\n",
    "Weâ€™ll learn:\n",
    "- What the perceptron is.\n",
    "- Why it works for classification.\n",
    "- How training happens with gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1af4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf9889",
   "metadata": {},
   "source": [
    "## 1) Biological Inspiration â€” WHY a \"neuron\"?\n",
    "\n",
    "- A biological neuron receives many inputs.\n",
    "- If inputs are strong enough, it \"fires\" (outputs 1), otherwise stays silent (0).\n",
    "- The perceptron mimics this:\n",
    "  $\n",
    "  y = \\text{step}(w \\cdot x + b)\n",
    "  $\n",
    "\n",
    "ðŸ‘‰ WHY important?\n",
    "This gave birth to the idea that **learning = adjusting weights** on inputs to change decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62579e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step activation function (hard threshold)\n",
    "def step_function(x):\n",
    "    return torch.tensor(1.0) if x >= 0 else torch.tensor(0.0)\n",
    "\n",
    "print(\"step(-2) =\", step_function(-2))\n",
    "print(\"step(3)  =\", step_function(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb46059",
   "metadata": {},
   "source": [
    "## 2) Perceptron Model â€” the math\n",
    "\n",
    "Equation:\n",
    "$\n",
    "y = \\text{step}(w \\cdot x + b)\n",
    "$\n",
    "\n",
    "- **w** = weights (how important each feature is).\n",
    "- **b** = bias (threshold shift).\n",
    "- **step** = activation (0 or 1).\n",
    "\n",
    "ðŸ‘‰ WHY?\n",
    "This transforms linear regression into a **binary classifier**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45352134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronManual:\n",
    "    def __init__(self, n_inputs):\n",
    "        self.w = torch.randn(n_inputs, requires_grad=True)\n",
    "        self.b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.dot(self.w, x) + self.b\n",
    "        return step_function(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097977d",
   "metadata": {},
   "source": [
    "## 3) Toy Example â€” AND Logic Gate\n",
    "\n",
    "The perceptron was originally tested on simple logic gates.\n",
    "\n",
    "Truth table (AND):\n",
    "- (0,0) â†’ 0\n",
    "- (0,1) â†’ 0\n",
    "- (1,0) â†’ 0\n",
    "- (1,1) â†’ 1\n",
    "\n",
    "ðŸ‘‰ WHY?\n",
    "Because if perceptron can learn logic gates, it proves it can separate categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: AND gate\n",
    "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "y = torch.tensor([[0.],[0.],[0.],[1.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20202c38",
   "metadata": {},
   "source": [
    "## 4) Training Perceptron â€” WHY gradient descent?\n",
    "\n",
    "The original Rosenblatt perceptron used a manual update rule.  \n",
    "Today, we use **gradient descent** (like in regression), but with a **sigmoid** instead of step.  \n",
    "\n",
    "ðŸ‘‰ WHY sigmoid?\n",
    "- Step is not differentiable â†’ cannot use gradient descent.\n",
    "- Sigmoid is smooth approximation â†’ allows training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ee224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronNN(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_inputs, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a19d36",
   "metadata": {},
   "source": [
    "## 5) Training on AND Gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceptronNN(2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss (AND gate)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4311bf7",
   "metadata": {},
   "source": [
    "## 6) Predictions â€” did it learn?\n",
    "\n",
    "ðŸ‘‰ WHY check predictions?  \n",
    "Loss decreasing is not enough â€” we must confirm model decisions match truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Predictions:\", model(X).round().view(-1).tolist())\n",
    "    print(\"Targets:    \", y.view(-1).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0145a2",
   "metadata": {},
   "source": [
    "## 7) Visualizing Decision Boundary\n",
    "\n",
    "ðŸ‘‰ WHY?  \n",
    "A picture shows how perceptron separates inputs into 0 vs 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    grid = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, levels=[0,0.5,1], cmap=\"coolwarm\", alpha=0.6)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y.view(-1), cmap=\"coolwarm\", edgecolor=\"k\")\n",
    "    plt.title(\"Decision boundary\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2726204",
   "metadata": {},
   "source": [
    "## 8) Limitations of Perceptron\n",
    "\n",
    "- Works only for **linearly separable problems** (AND, OR).\n",
    "- Fails for XOR (not separable by straight line).  \n",
    "\n",
    "ðŸ‘‰ WHY important?  \n",
    "This limitation caused the **AI winter** (1969, Minsky & Papert proved perceptron canâ€™t solve XOR).  \n",
    "Solution: add **multiple layers** â†’ Multilayer Perceptron (next lesson).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d958a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "y_xor = torch.tensor([[0.],[1.],[1.],[0.]])\n",
    "\n",
    "# Train perceptron\n",
    "model_xor = PerceptronNN(2)\n",
    "optimizer = optim.SGD(model_xor.parameters(), lr=0.1)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model_xor(X_xor)\n",
    "    loss = criterion(y_pred, y_xor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model_xor(X_xor).round().view(-1).tolist()\n",
    "\n",
    "print(\"Predictions for XOR:\", preds)\n",
    "print(\"Targets:            \", y_xor.view(-1).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c9336",
   "metadata": {},
   "source": [
    "## âœ… Summary â€” Why Perceptron Matters\n",
    "\n",
    "- First neural network model (1958).\n",
    "- Introduced idea: weights + bias + activation.  \n",
    "- Shows how classification works.  \n",
    "- Limitation: cannot solve XOR â†’ motivated **multilayer networks**.  \n",
    "\n",
    "ðŸš€ Next Lesson: **Multilayer Perceptron (MLP)** â€” how adding layers solves XOR and builds modern deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01735a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
