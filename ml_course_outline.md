# Chu·ªói B√†i Gi·∫£ng Machine Learning v·ªõi PyTorch
## T·ª´ Nh·ªØng Kh·ªëi X√¢y D·ª±ng Nh·ªè Nh·∫•t

---

## **PH·∫¶N 1: FOUNDATIONS (N·ªÅn T·∫£ng)**

### B√†i 1: Tensor - Kh·ªëi X√¢y D·ª±ng C∆° B·∫£n Nh·∫•t
- Tensor l√† g√¨? T·∫°i sao c·∫ßn tensor?
- T·∫°o tensor, c√°c ph√©p to√°n c∆° b·∫£n
- Reshape, indexing, slicing
- **Th·ª±c h√†nh**: T√≠nh to√°n v·ªõi ma tr·∫≠n

### B√†i 2: Automatic Differentiation (T√≠nh ƒê·∫°o H√†m T·ª± ƒê·ªông)  
- Gradient l√† g√¨? T·∫°i sao c·∫ßn gradient?
- `requires_grad=True` v√† `.backward()`
- Chain rule trong th·ª±c t·∫ø
- **Th·ª±c h√†nh**: T√≠nh gradient c·ªßa h√†m ƒë∆°n gi·∫£n

### B√†i 3: Linear Regression - M√¥ H√¨nh ƒê·∫ßu Ti√™n
- T·ª´ c√¥ng th·ª©c to√°n h·ªçc ƒë·∫øn code
- `y = wx + b` - hi·ªÉu t·ª´ng th√†nh ph·∫ßn
- Loss function, optimizer
- **Th·ª±c h√†nh**: D·ª± ƒëo√°n gi√° nh√† v·ªõi 1 feature

---

## **PH·∫¶N 2: NEURAL NETWORK BASICS**

### B√†i 4: Perceptron - Neuron ƒê∆°n Gi·∫£n Nh·∫•t
- 1 neuron l√†m vi·ªác nh∆∞ th·∫ø n√†o?
- Activation function (sigmoid, tanh, ReLU)
- Forward pass t·ª´ng b∆∞·ªõc
- **Th·ª±c h√†nh**: Binary classification v·ªõi 1 neuron

### B√†i 5: Multi-Layer Perceptron (MLP)
- Gh√©p nhi·ªÅu neuron l·∫°i
- Hidden layers l√† g√¨?
- Backpropagation gi·∫£i th√≠ch ƒë∆°n gi·∫£n
- **Th·ª±c h√†nh**: Ph√¢n lo·∫°i h√¨nh ·∫£nh MNIST

### B√†i 6: PyTorch nn.Module
- C·∫•u tr√∫c c·ªßa m·ªôt neural network class
- `__init__()` v√† `forward()`
- Parameters vs buffers
- **Th·ª±c h√†nh**: T·ª± x√¢y d·ª±ng MLP class

---

## **PH·∫¶N 3: ADVANCED ARCHITECTURES**

### B√†i 7: Convolutional Neural Networks (CNN)
- Convolution operation - b·ªô l·ªçc l√† g√¨?
- Pooling, stride, padding
- T·ª´ feature extraction ƒë·∫øn classification
- **Th·ª±c h√†nh**: Image classification v·ªõi CNN

### B√†i 8: Recurrent Neural Networks (RNN) - Phi√™n B·∫£n ƒê∆°n Gi·∫£n
- T·∫°i sao c·∫ßn "nh·ªõ"? 
- Hidden state step by step
- X·ª≠ l√Ω sequence d·ªØ li·ªáu
- **Th·ª±c h√†nh**: D·ª± ƒëo√°n chu·ªói s·ªë ƒë∆°n gi·∫£n

### B√†i 9: Long Short-Term Memory (LSTM)
- V·∫•n ƒë·ªÅ vanishing gradient c·ªßa RNN
- Cell state vs hidden state
- Gates mechanism (forget, input, output)
- **Th·ª±c h√†nh**: Text sentiment analysis

---

## **PH·∫¶N 4: PRACTICAL ML**

### B√†i 10: Data Loading & Preprocessing
- Dataset v√† DataLoader
- Data augmentation
- Normalization, standardization
- **Th·ª±c h√†nh**: X·ª≠ l√Ω dataset th·ª±c t·∫ø

### B√†i 11: Training Loop & Validation
- Training loop anatomy
- Overfitting, underfitting
- Validation strategies
- **Th·ª±c h√†nh**: Model evaluation ƒë√∫ng c√°ch

### B√†i 12: Transfer Learning
- Pre-trained models
- Fine-tuning vs feature extraction  
- Domain adaptation
- **Th·ª±c h√†nh**: S·ª≠ d·ª•ng ResNet cho custom dataset

---

## **PH·∫¶N 5: ADVANCED TOPICS**

### B√†i 13: Attention Mechanism
- Attention l√† g√¨? T·∫°i sao quan tr·ªçng?
- Query, Key, Value concept
- Self-attention c∆° b·∫£n
- **Th·ª±c h√†nh**: Simple attention model

### B√†i 14: Transformer Architecture
- Multi-head attention
- Positional encoding
- Encoder-decoder structure
- **Th·ª±c h√†nh**: Mini transformer cho sequence tasks

### B√†i 15: Generative Models Basics
- Autoencoder
- Variational Autoencoder (VAE) 
- Generative Adversarial Networks (GAN) concept
- **Th·ª±c h√†nh**: Image generation ƒë∆°n gi·∫£n

---

## **C√ÅCH TI·∫æP C·∫¨N H·ªåC T·∫¨P:**

### üéØ **M·ªói B√†i S·∫Ω C√≥:**
1. **Concept**: Gi·∫£i th√≠ch l√Ω thuy·∫øt b·∫±ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n
2. **Visual**: H√¨nh v·∫Ω, diagram minh h·ªça  
3. **Code**: T·ª´ code c∆° b·∫£n nh·∫•t, gi·∫£i th√≠ch t·ª´ng d√≤ng
4. **Practice**: B√†i t·∫≠p th·ª±c h√†nh nh·ªè
5. **Real Example**: ·ª®ng d·ª•ng th·ª±c t·∫ø

### üìö **H·ªçc T·∫≠p Tu·∫ßn T·ª±:**
- M·ªói b√†i x√¢y d·ª±ng d·ª±a tr√™n b√†i tr∆∞·ªõc
- Kh√¥ng b·ªè qua b√†i n√†o
- Code t·ª´ ƒë∆°n gi·∫£n ‚Üí ph·ª©c t·∫°p
- Hi·ªÉu **t·∫°i sao** tr∆∞·ªõc khi h·ªçc **c√°ch l√†m**

### ‚ö° **Nguy√™n T·∫Øc "Building Blocks":**
- M·ªói concept l√† 1 kh·ªëi nh·ªè
- K·∫øt h·ª£p c√°c kh·ªëi ƒë·ªÉ t·∫°o h·ªá th·ªëng l·ªõn  
- Hi·ªÉu t·ª´ng kh·ªëi tr∆∞·ªõc khi k·∫øt h·ª£p
- Code minh h·ªça r√µ r√†ng cho t·ª´ng kh·ªëi

---

## **B·∫ÆT ƒê·∫¶U T·ª™ ƒê√ÇU?**

**B√†i ƒë·∫ßu ti√™n** s·∫Ω l√† **"Tensor - Kh·ªëi X√¢y D·ª±ng C∆° B·∫£n Nh·∫•t"**

T√¥i s·∫Ω gi·∫£i th√≠ch:
- Tensor l√† g√¨ (nh∆∞ array nh∆∞ng m·∫°nh h∆°n)
- T·∫°i sao ML c·∫ßn tensor  
- C√°c ph√©p to√°n c∆° b·∫£n
- Code examples t·ª´ng b∆∞·ªõc m·ªôt

**B·∫°n c√≥ mu·ªën b·∫Øt ƒë·∫ßu v·ªõi B√†i 1 ngay kh√¥ng?** üöÄ