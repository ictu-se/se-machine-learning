{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf442dc",
   "metadata": {},
   "source": [
    "# üìò Lesson 5 ‚Äî Multilayer Perceptron (MLP): Building Deep Networks\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why this lesson matters\n",
    "- The single perceptron can only solve **linearly separable problems** (AND, OR).  \n",
    "- It **fails on XOR** ‚Üí this caused the ‚ÄúAI Winter‚Äù in the 1970s.  \n",
    "- Solution: **add hidden layers** ‚Üí Multilayer Perceptron (MLP).  \n",
    "\n",
    "üëâ MLP is the ancestor of all modern deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0b490",
   "metadata": {},
   "source": [
    "## 1) What is an MLP?\n",
    "\n",
    "An **MLP** is:\n",
    "- Input layer ‚Üí Hidden layer(s) ‚Üí Output layer\n",
    "- Each layer: linear transformation + non-linear activation\n",
    "\n",
    "$\n",
    "y = f(W_2 \\cdot f(W_1 x + b_1) + b_2)\n",
    "$\n",
    "\n",
    "üëâ WHY hidden layers?\n",
    "- One layer = only linear decision boundary.  \n",
    "- Multiple layers + nonlinear activation = can approximate any function (Universal Approximation Theorem).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e03935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4)   # 2 inputs ‚Üí 4 hidden units\n",
    "        self.out = nn.Linear(4, 1)      # 4 hidden ‚Üí 1 output\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        x = self.activation(self.out(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87a492",
   "metadata": {},
   "source": [
    "## 2) Dataset: XOR Problem\n",
    "\n",
    "We use XOR, which **cannot** be solved by a single perceptron.  \n",
    "This shows the real power of MLPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "y = torch.tensor([[0.],[1.],[1.],[0.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7a043",
   "metadata": {},
   "source": [
    "## 3) Training MLP on XOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf57fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(5000):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss (XOR with MLP)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ed637",
   "metadata": {},
   "source": [
    "## 4) Predictions ‚Äî did it learn XOR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(X).round().view(-1).tolist()\n",
    "print(\"Predictions:\", preds)\n",
    "print(\"Targets:    \", y.view(-1).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a03c3",
   "metadata": {},
   "source": [
    "## 5) Visualizing Decision Boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    grid = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, levels=[0,0.5,1], cmap=\"coolwarm\", alpha=0.6)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y.view(-1), cmap=\"coolwarm\", edgecolor=\"k\")\n",
    "    plt.title(\"MLP Decision Boundary for XOR\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6049d",
   "metadata": {},
   "source": [
    "## 6) Why MLP works for XOR (the key insight)\n",
    "\n",
    "- A perceptron fails because XOR is **not linearly separable**.  \n",
    "- With a hidden layer:\n",
    "  - First layer creates intermediate features (like AND, OR).  \n",
    "  - Second layer combines them to represent XOR.  \n",
    "\n",
    "üëâ This shows the **power of representation learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff27206",
   "metadata": {},
   "source": [
    "## 7) Extending MLP\n",
    "\n",
    "- Add more hidden layers ‚Üí deeper networks.  \n",
    "- Change activation (ReLU, Tanh).  \n",
    "- Use optimizers beyond SGD (Adam).  \n",
    "\n",
    "üëâ WHY?\n",
    "This is the foundation of **deep learning**.  \n",
    "Every CNN, RNN, Transformer is an advanced MLP variant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4303c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
