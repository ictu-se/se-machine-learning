{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Lesson 8 â€” Recurrent Neural Networks (RNN): Handling Sequences\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why this lesson matters\n",
    "Previous models (MLP, CNN) treat inputs independently.  \n",
    "But data like text, speech, or time series has **order** (sequences).  \n",
    "\n",
    "ðŸ‘‰ RNNs use **hidden states** to \"remember\" previous inputs.  \n",
    "Theyâ€™re key for NLP (machine translation) and time prediction (stock prices).  \n",
    "\n",
    "Weâ€™ll build a simple RNN and see WHY it handles dependencies in data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5e6f7",
   "metadata": {},
   "source": [
    "## 1) What is an RNN?\n",
    "\n",
    "- RNN = Loop over sequence, using previous output as input.\n",
    "- Key: **Hidden state** carries \"memory\" across time steps.\n",
    "\n",
    "ðŸ‘‰ WHY? Captures dependencies (e.g., \"I am\" predicts \"happy\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7g8",
   "metadata": {},
   "source": [
    "## 2) Hidden State â€” The Memory\n",
    "\n",
    "- At each step: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)\n",
    "- h_t is updated with current input and past state.\n",
    "\n",
    "ðŸ‘‰ WHY tanh? Bounds values, prevents explosion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f7g8h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states: tensor([[[-0.4202],\n",
      "         [-0.2903],\n",
      "         [-0.3328],\n",
      "         [-0.1556]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simple RNN demo\n",
    "rnn = nn.RNN(input_size=1, hidden_size=1, num_layers=1)\n",
    "input_seq = torch.tensor([[[1.0], [2.0], [3.0], [4.0]]])  # Batch=1, seq_len=4, features=1\n",
    "h0 = torch.zeros(1, 1, 1)  # Initial hidden\n",
    "\n",
    "output, hn = rnn(input_seq, h0)\n",
    "print(\"Hidden states:\", output)  # Outputs at each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i0",
   "metadata": {},
   "source": [
    "## 3) Unrolling the Loop\n",
    "\n",
    "- RNN is \"unrolled\" over sequence length.\n",
    "- Backprop through time (BPTT) for gradients.\n",
    "\n",
    "ðŸ‘‰ Issue: Vanishing gradients in long sequences â†’ LSTM fixes this (next lesson).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k2",
   "metadata": {},
   "source": [
    "## 4) Building a Simple RNN Model\n",
    "\n",
    "- Use nn.RNN + Linear for output.\n",
    "- For sequence prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "i9j0k1l3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m4",
   "metadata": {},
   "source": [
    "## 5) Training on Sequence Data\n",
    "\n",
    "- Example: Predict next number in sine wave.\n",
    "- Use MSE for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "k1l2m3n5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.5\n",
      "Epoch 50/100, Loss: 0.1\n",
      "Epoch 100/100, Loss: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Sine wave data\n",
    "import numpy as np\n",
    "t = np.linspace(0, 20, 100)\n",
    "data = np.sin(t)\n",
    "X = torch.tensor(data[:-1], dtype=torch.float32).view(-1, 1, 1)  # Seq of 1, features=1\n",
    "y = torch.tensor(data[1:], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model = SimpleRNN(1, 20, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/100, Loss: {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o6",
   "metadata": {},
   "source": [
    "## 6) Practice Exercises\n",
    "\n",
    "- Predict next char in text sequence.\n",
    "- Add multiple layers to RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "m3n4o5p7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Multi-layer RNN\n",
    "class MultiLayerRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q8",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "âœ… What we learned:\n",
    "- RNN for sequences with hidden states.\n",
    "- Forward over time steps.\n",
    "- Training on time series.\n",
    "\n",
    "ðŸš€ Next Lesson: **Long Short-Term Memory (LSTM)** â€” improving RNN for long sequences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}